{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "manufactured-short",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(Text processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-adobe",
   "metadata": {},
   "source": [
    "출처 : https://wikidocs.net/21694"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-mistress",
   "metadata": {},
   "source": [
    "NLTK에서는 영어 코러스에 품사 태깅 기능을 지원하고 있다. 품사를 어떻게 명명하고, 태깅하는지의 기준은 여러가지가 있는데, NLTK에서는 Penn Treebank POS Tags라는 기준을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-grace",
   "metadata": {},
   "source": [
    "## 0. NLTK와 NLTK Data 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binding-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\soyeon\\python2021\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\soyeon\\python2021\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\soyeon\\python2021\\lib\\site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex in c:\\users\\soyeon\\python2021\\lib\\site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soyeon\\python2021\\lib\\site-packages (from nltk) (4.56.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "determined-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artistic-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-consultancy",
   "metadata": {},
   "source": [
    "## 1. 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pursuant-armor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D', ',', 'students']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D, students\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D', 'NNP'),\n",
       " (',', ','),\n",
       " ('students', 'NNS')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-drill",
   "metadata": {},
   "source": [
    "#### Penn Tree bank POG Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-hardwood",
   "metadata": {},
   "source": [
    "    PRP: 인칭대명사\n",
    "    VBP: 동사\n",
    "    RB: 부사\n",
    "    VBG: 현재부사\n",
    "    IN: 전치사\n",
    "    NNP: 고유 명사\n",
    "    NNS: 복수형 명사\n",
    "    CC: 접속사\n",
    "    DT: 관사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-madness",
   "metadata": {},
   "source": [
    "한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 이용할 수 있다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-penetration",
   "metadata": {},
   "source": [
    "한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행하게 됨을 뜻한다. \n",
    "\n",
    "이 중에서 Okt와 꼬꼬마를 통해서 토큰화를 수행해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bored-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\soyeon\\python2021\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (1.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (4.6.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-episode",
   "metadata": {},
   "source": [
    "#### Okt 형태소 분석기로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fiscal-cooper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # morphs : 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italian-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # pos : 품사 태깅(Part-of-speech tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collectible-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 코딩ㄹ한 당신, 연휴에는 여행을 가봐요\"))  # nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-andrews",
   "metadata": {},
   "source": [
    "#### 꼬꼬마 형태소 분석기로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "herbal-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma=Kkma()\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # morphs : 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "first-subject",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # pos : 품사 태깅(Part-of-speech tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liberal-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-robert",
   "metadata": {},
   "source": [
    "## 2. 정제(Cleaning)와 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-legend",
   "metadata": {},
   "source": [
    "* 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "\n",
    "* 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-small",
   "metadata": {},
   "source": [
    "## 3. 어간 추출(Stemming)과 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-seeker",
   "metadata": {},
   "source": [
    "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것이다. \n",
    "\n",
    "형태소란 '의미를 가진 가장 작은 단위'를 뜻하고, 형태학(morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-preservation",
   "metadata": {},
   "source": [
    "형태소의 두 가지 종류\n",
    "\n",
    "* 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "* 접사(affix) : 단어에 추가적인 의미를 주는 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-parks",
   "metadata": {},
   "source": [
    "#### 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smooth-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-bangladesh",
   "metadata": {},
   "source": [
    "표제어 추출기(lemmatizer)는 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recent-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "organic-smell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minor-blogger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-opening",
   "metadata": {},
   "source": [
    "#### 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-exhibit",
   "metadata": {},
   "source": [
    "예제 : 포터 알고리즘(Potter Algorithm)에 아래의 Text를 넣어보자.\n",
    "\n",
    "input : This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lyric-treasurer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consistent-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-vertical",
   "metadata": {},
   "source": [
    "포터 알고리즘에서의 어간 추출은 단순 규칙에 기반하여 이루어지기 때문에 사전에 없는 단어들도 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-celebration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-morning",
   "metadata": {},
   "source": [
    "Porter 알고리즘의 상세 규칙 : https://tartarus.org/martin/PorterStemmer/index.html\n",
    "\n",
    "어간 추출 속도가 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-picking",
   "metadata": {},
   "source": [
    "#### 랭커스터 스태머(Lancaster Stemmer) 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "peripheral-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘으로 어간 추출 했을 때\n",
    "from nltk.stem import PorterStemmer\n",
    "s=PorterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dependent-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "# 랭커스터 스태머 알고리즘으로 어간 추출 했을 때\n",
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-reach",
   "metadata": {},
   "source": [
    "#### 한국어 불규칙 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-niagara",
   "metadata": {},
   "source": [
    "링크 : https://namu.wiki/w/한국어/불규칙%20활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-allah",
   "metadata": {},
   "source": [
    "## 4. 불용어(Stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-pointer",
   "metadata": {},
   "source": [
    "불용어(stopword) : 실제 의미 분석을 하는 데에 거의 기여하지 않는 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-sport",
   "metadata": {},
   "source": [
    "#### NLTK에서 불용어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raising-damage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-certification",
   "metadata": {},
   "source": [
    "#### NLTK로 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surgical-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-watts",
   "metadata": {},
   "source": [
    "#### 한국어 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "joined-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-formula",
   "metadata": {},
   "source": [
    "#### 보편적으로 사용하는 한국어 불용어 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-amsterdam",
   "metadata": {},
   "source": [
    "링크 : https://www.ranks.nl/stopwords/korean\n",
    "\n",
    "추가 참고 가능한 한국어 불용어 리스트 : https://bab2min.tistory.com/544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-ordinary",
   "metadata": {},
   "source": [
    "## 5. 정규 표현식(Regular Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laden-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-teach",
   "metadata": {},
   "source": [
    "* .은 한 개의 임의의 문자를 나타냅니다. \n",
    "\n",
    "예를 들어서 정규 표현식이 a.c라고 합시다. a와 c 사이에는 어떤 1개의 문자라도 올 수 있습니다. 즉, akc, azc, avc, a5c, a!c와 같은 형태는 모두 a.c의 정규 표현식과 매치됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "driven-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=re.compile(\"a.c\")\n",
    "r.search(\"kkk\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "varying-carol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-causing",
   "metadata": {},
   "source": [
    "위의 코드는 search의 입력으로 들어오는 문자열에 정규표현식 패턴 a.c이 존재하는지를 확인하는 코드입니다. (.)은 어떤 문자로도 인식될 수 있기 때문에 abc라는 문자열은 a.c라는 정규 표현식 패턴으로 매치되는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-pocket",
   "metadata": {},
   "source": [
    "* ? : ? 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있는 경우\n",
    "\n",
    "ab?c => abc 또는 ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "horizontal-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=re.compile(\"ab?c\")\n",
    "r.search(\"abbc\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secret-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dietary-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-deposit",
   "metadata": {},
   "source": [
    "* 별 : 바로 앞의 문자가 0개 이상일 경우 \n",
    "\n",
    "ab*c => ac, abc, abbc, abbbc, abbbbc, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-portfolio",
   "metadata": {},
   "source": [
    "* +기호 : 앞의 문자가 최소 1개 이상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-jewelry",
   "metadata": {},
   "source": [
    "* ^기호 : 시작되는 글자 지정\n",
    "\n",
    "^a => a로 시작되는 문자열만을 찾아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-tragedy",
   "metadata": {},
   "source": [
    "* {숫자} : 문자를 해당 숫자만큼 반복한 것\n",
    "\n",
    "ab{2}c => abbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-franklin",
   "metadata": {},
   "source": [
    "* {숫자1, 숫자2} : 해당 문자를 숫자 1 이상 숫자 2 이하 반복\n",
    "\n",
    "* {숫자,} : 해당 문자를 숫자 이상만큼 반복\n",
    "\n",
    "* [] : [] 안으 문자들 중 한 개의 문자와 매치\n",
    "\n",
    "* [^문자] : ^기호 뒤에 뭍은 문자들을 제외한 모든 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-wonder",
   "metadata": {},
   "source": [
    "#### 정규 표현식 모듈 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-dialogue",
   "metadata": {},
   "source": [
    "* re.search() : 정규 표현식 전체에 대해 문자열이 매치하는지\n",
    "\n",
    "* re.match() : 문자열의 첫 부분부터 정규 표현식과 매치하는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impressed-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"ab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colonial-charlotte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"kkkabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.match(\"kkkabc\") # 아무런 결과도 출력되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-cowboy",
   "metadata": {},
   "source": [
    "* re.split() : 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴 (토큰화에 유용하게 쓰임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intellectual-anger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-semiconductor",
   "metadata": {},
   "source": [
    "* re.findall() : 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴 (단, 매치되는 문자열이 없다면 빈 리스트를 리턴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chronic-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"이름 : 김철수 전화번호 : 010 - 1234 - 1234 나이 : 30 성별 : 남\"\n",
    "re.findall(\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-amount",
   "metadata": {},
   "source": [
    "* re.sub() : 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock-newsletter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "re.sub('[^a-zA-Z]',' ',text)  # 특수문자 제거 - 알파벡벳 외의 문자는 공백으로 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-watts",
   "metadata": {},
   "source": [
    "#### 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equipped-nepal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  \n",
    "\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"  \n",
    "\n",
    "re.split('\\s+', text)  # '\\s+'는 최소 1개 이상의 공백 패턴을 찾아내는 정규표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "welsh-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+', text)  # 숫자만 뽑아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "detected-driving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]',text)  # 대문자인 행의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "configured-section",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}',text)  # 대문자가 연속적으로 4번 등장하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "overall-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+',text)  # 이름에 대한 행의 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-watts",
   "metadata": {},
   "source": [
    "#### 정규 표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sudden-marble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")  # \\+는 문자 또는 숫자가 1개 이상인 경우를 인식하는 코드 (구두점을 제외한 단어들만 가지고 토큰화)\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "disciplinary-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\s]+\", gaps=True)  # 공백을 기준으로 문장 토큰화\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-galaxy",
   "metadata": {},
   "source": [
    "위의 경우, 아포스트로피나 온점을 제외하지 않고 토큰화 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-member",
   "metadata": {},
   "source": [
    "## 6. 정수 인코딩(Integer Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-cedar",
   "metadata": {},
   "source": [
    "#### dictionary 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impressive-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inner-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wanted-force",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attended-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 정제와 단어 토큰화\n",
    "vocab = {}  # 파이썬의 dictionary 자료형 (key:value)\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # 단어 토큰화 수행\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word = word.lower()  # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words:  # 불용어가 아니라면\n",
    "            if len(word) > 2:  # 단어 길이가 2 이하가 아니라면\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1  # 각 단어의 빈도수 측정\n",
    "    sentences.append(result)\n",
    "print(sentences)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfied-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sensitive-australian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fewer-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 단어일수록 낮은 정수 인덱스 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for(word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developed-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 상위 5개 단어만 사용\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1]  # 인덱스가 5 초과인 단어 제거\n",
    "\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]  # 해당 단어에 대한 인덱스 정보 삭제\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-typing",
   "metadata": {},
   "source": [
    "* OOV(Out-Of-Vocabulary) : 단어 집합에 존재하지 않는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "detected-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어집합에 없는 단어들은 OOV 인덱스로 인코딩\n",
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "working-hungarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "material-diploma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# sentences의 모든 단어들을 맵핑되는 점수로 인코딩\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-consolidation",
   "metadata": {},
   "source": [
    "#### Counter 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "packed-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "insured-authorization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "desirable-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# sentences 단어들을 하나의 리스트로 만들기\n",
    "words = sum(sentences, [])\n",
    "# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "personalized-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "developed-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expensive-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "promotional-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-eight",
   "metadata": {},
   "source": [
    "#### NLTK의 FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "suited-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interesting-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...\n",
    "vocab = FreqDist(np.hstack(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "informal-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "indian-trailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "trained-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 부여\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-madrid",
   "metadata": {},
   "source": [
    "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "parental-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "test=['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
    "    print(\"value : {}, index: {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-wilderness",
   "metadata": {},
   "source": [
    "#### 케라스(Keras)의 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "official-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adjacent-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "engaged-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "serious-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# 각 단어의 인덱스\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "coated-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 각 단어의 빈도수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "gorgeous-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스로 변환\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "standing-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 단어 사용 재정의\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # num_words는 숫자를 0부터 카운트함.\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "antique-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "valuable-insulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "palestinian-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))  # 상위 5개 단어 사용은 texts_to_sequences 에서만 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "parallel-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# word_index와 word_counts에서도 지정된 num_words 만큼의 단어만 남기고 싶다면?\n",
    "\n",
    "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for w in words_frequency:\n",
    "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "antique-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 토크나어저는 기본적으로 OOV에 대해서 단어를 정수를 바꾸는 과정에서 아예 단어를 제거함.\n",
    "# 그래도 OOV로 간주하여 남기고 싶다면?\n",
    "\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efficient-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "# 만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.\n",
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fewer-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-cartoon",
   "metadata": {},
   "source": [
    "## 7. 패딩(Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-cheat",
   "metadata": {},
   "source": [
    "자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다. 실습을 통해 이해해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "opposite-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "swiss-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "commercial-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "statutory-taxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 2, 6],\n",
       "       [0, 0, 0, 0, 2, 1, 6],\n",
       "       [0, 0, 0, 0, 2, 4, 6],\n",
       "       [0, 0, 0, 0, 0, 1, 3],\n",
       "       [0, 0, 0, 3, 5, 4, 3],\n",
       "       [0, 0, 0, 0, 0, 4, 3],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 3],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [0, 0, 0, 2, 1, 4, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스로 패딩\n",
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "athletic-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding = 'post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "advisory-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0],\n",
       "       [2, 4, 6, 0, 0],\n",
       "       [1, 3, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0],\n",
       "       [4, 3, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 3, 0, 0],\n",
       "       [4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maxlen 만큼의 길이로 제한\n",
    "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "extraordinary-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# 0이 아니라 다른 숫자로 패딩\n",
    "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\n",
    "print(last_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "opened-recovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6, 15, 15, 15, 15, 15],\n",
       "       [ 2,  1,  6, 15, 15, 15, 15],\n",
       "       [ 2,  4,  6, 15, 15, 15, 15],\n",
       "       [ 1,  3, 15, 15, 15, 15, 15],\n",
       "       [ 3,  5,  4,  3, 15, 15, 15],\n",
       "       [ 4,  3, 15, 15, 15, 15, 15],\n",
       "       [ 2,  5,  1, 15, 15, 15, 15],\n",
       "       [ 2,  5,  1, 15, 15, 15, 15],\n",
       "       [ 2,  5,  3, 15, 15, 15, 15],\n",
       "       [ 1,  1,  4,  3,  1,  2,  1],\n",
       "       [ 2,  1,  4,  1, 15, 15, 15]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding = 'post', value = last_value)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-opposition",
   "metadata": {},
   "source": [
    "## 8. 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-senate",
   "metadata": {},
   "source": [
    "원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다. \n",
    "\n",
    "이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 합니다.\n",
    "\n",
    "\n",
    "원-핫 인코딩을 두 가지 과정으로 정리해보겠습니다.\n",
    "\n",
    "(1) 각 단어에 고유한 인덱스를 부여합니다. (정수 인코딩)\n",
    "\n",
    "(2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "photographic-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "funded-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "creative-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded=t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "solid-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩 수행\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-messenger",
   "metadata": {},
   "source": [
    "### 원핫 인코딩의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-vulnerability",
   "metadata": {},
   "source": [
    "1. 단어의 개수가 늘어날수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다.\n",
    "\n",
    "2. 원-핫 벡터는 단어의 유사도를 표현하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-practitioner",
   "metadata": {},
   "source": [
    "해결 => 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화하는 기법\n",
    "\n",
    "1. 카운트 기반 벡터화 (LSA, HAL)\n",
    "\n",
    "2. 예측 기반 벡터화 (NNLM, RNNLM, Word2Vec, FastText)\n",
    "\n",
    "3. 둘 다 사용 (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-lambda",
   "metadata": {},
   "source": [
    "## 9. 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-hardware",
   "metadata": {},
   "source": [
    "#### 사이킷런으로 분리하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "scheduled-illustration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
    "print(X)\n",
    "print(list(y)) #레이블 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "missing-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "brown-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "#3분의 1만 test 데이터로 지정.\n",
    "#random_state 지정으로 인해 순서가 섞인 채로 훈련 데이터와 테스트 데이터가 나눠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "concerned-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "[[8 9]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-kenya",
   "metadata": {},
   "source": [
    "#### 수동으로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "clinical-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, y = np.arange(0,24).reshape((12,2)), range(12)\n",
    "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "structural-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "n_of_train = int(len(X) * 0.8) # 데이터의 전체 길이의 80%에 해당하는 길이값을 구한다.\n",
    "n_of_test = int(len(X) - n_of_train) # 전체 길이에서 80%에 해당하는 길이를 뺀다.\n",
    "print(n_of_train)\n",
    "print(n_of_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "earlier-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[n_of_train:] #전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
    "y_test = y[n_of_train:] #전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
    "X_train = X[:n_of_train] #전체 데이터 중에서 80%만큼 앞의 데이터 저장\n",
    "y_train = y[:n_of_train] #전체 데이터 중에서 80%만큼 앞의 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "disabled-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 19]\n",
      " [20 21]\n",
      " [22 23]]\n",
      "[9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-saint",
   "metadata": {},
   "source": [
    "## 10. 한국어 전처리 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "underlying-discrimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to c:\\users\\soyeon\\appdata\\local\\temp\\pip-req-build-2kebuvnc\n",
      "Requirement already satisfied: tensorflow==2.4.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from pykospacing==0.4) (2.4.0)\n",
      "Requirement already satisfied: keras>=2.4.3 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from pykospacing==0.4) (2.4.3)\n",
      "Requirement already satisfied: h5py==2.10.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from pykospacing==0.4) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from h5py==2.10.0->pykospacing==0.4) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\soyeon\\python2021\\lib\\site-packages (from h5py==2.10.0->pykospacing==0.4) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n",
      "Collecting argparse>=1.4.0\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\soyeon\\python2021\\lib\\site-packages (from keras>=2.4.3->pykospacing==0.4) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from keras>=2.4.3->pykospacing==0.4) (1.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (49.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\soyeon\\python2021\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n",
      "Building wheels for collected packages: pykospacing\n",
      "  Building wheel for pykospacing (setup.py): started\n",
      "  Building wheel for pykospacing (setup.py): finished with status 'done'\n",
      "  Created wheel for pykospacing: filename=pykospacing-0.4-py3-none-any.whl size=2255491 sha256=1f9b858122fa866807e2585074d1341e6763811c556cbe37e6d3b97edb264d05\n",
      "  Stored in directory: C:\\Users\\Soyeon\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-tspet9ax\\wheels\\79\\a0\\33\\16f2cd03d21f76a663f5d69a0b96f0351335385349136fbd03\n",
      "Successfully built pykospacing\n",
      "Installing collected packages: argparse, pykospacing\n",
      "Successfully installed argparse-1.4.0 pykospacing-0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
